#  CNFinBench: Evaluating Expertise, Autonomy, and Integrity in Finance

**Language**: [English](README.md) | [ä¸­æ–‡](README_CN.md)

[![Paper](https://img.shields.io/badge/Paper-arXiv-red)](https://arxiv.org/abs/2512.09506)
[![Leaderboard](https://img.shields.io/badge/Leaderboard-OpenCompass-blue)](https://cnfinbench.opencompass.org.cn/home)

---

ğŸ“¹ **For clearer demo video, please click the link:** [Video](https://www.bilibili.com/video/BV1tCFKz7E5V/?spm_id_from=333.1368.list.card_archive.click&vd_source=59e46b06947602ba456024ab5036a801)  

https://github.com/user-attachments/assets/c15e95fb-8081-474b-936a-3686ac312c62
## ğŸ“£ News & Announcements

ğŸ“° **Media Coverage**  
- ğŸ‘‰ [CNFinBench Released: A New Benchmark for Financial LLM Safety Evaluation](https://mp.weixin.qq.com/s/z427UB6r6QPNyAX_Pfc0JA)
- ğŸ‘‰ [From Capabilities to Agents: How CNFinBench Evaluates Compliance and Safety of Financial LLMs](https://mp.weixin.qq.com/s/5UUBlklvoB67nQYOIXee7Q)


ğŸ“„ **Academic Release**  
- **Beyond Knowledge to Agency: Evaluating Expertise, Autonomy, and Integrity in Finance with CNFinBench**  
  ğŸ”— https://arxiv.org/abs/2512.09506

ğŸŒ **Online Leaderboard**  
- ğŸ”¥ Live leaderboard and model submission:   https://cnfinbench.opencompass.org.cn/home


---

## ğŸ“– What is CNFinBench?


**CNFinBench** is a comprehensive benchmark for evaluating **large language models and agentic systems** in **high-stakes financial scenarios**.

Unlike traditional textbook-style financial QA benchmarks, CNFinBench targets **real-world deployment risks** introduced by *high-privilege financial agents*, and systematically evaluates models along three orthogonal axes:

- **Expertise** â€“ professional financial knowledge and reasoning  
- **Autonomy** â€“ multi-step planning, tool use, and agent execution  
- **Integrity** â€“ safety, compliance, and robustness under adversarial interaction  

CNFinBench spans **29 fine-grained tasks**, grounded in certified regulatory corpora, real financial workflows, and multi-turn adversarial attack scenarios.

---

## ğŸ§© Task Taxonomy

CNFinBench decomposes financial intelligence into a three-dimensional evaluation space:

<div align="center">
<img align="center" src="assets/fig-task-taxonomy.png" width="80%"/>
</div>



### ğŸ“šExpertise â€“ Financial Capability

- Financial Knowledge Mastery
- Complex Logic Composition
- Contextual Analysis Resilience

### âš’ï¸ Autonomy â€“ Agentic Execution

- End-to-End Execution (Intent â†’ Plan â†’ Tool â†’ Verification)
- Strategic Planning & Reasoning
- Meta-cognitive Reliability

### ğŸ¥·ğŸ» Integrity â€“ Safety & Compliance

- Immediate Risk Interception
- Compliance Persistence
- Dynamic Adversarial Orchestration

---

## ğŸ” Multi-turn Safety Evaluation & HICS


To quantify **behavioral compliance degradation**, CNFinBench introduces:

### **Harmful Instruction Compliance Score (HICS)**

- Multi-dimensional, severity-aware safety metric  
- Tracks violation escalation across dialogue rounds  
- Supports interpretable rule-level deduction logs  
- Reveals **collapse rhythms** under different attack strategies  

---

### ğŸ— How is CNFinBench constructed?

CNFinBench adopts a multi-stage **data generation pipeline** that combines:

<div align="center">
<img align="center" src="assets/fig-data-gen.png" width="80%"/>
</div>

- **LLM-assisted synthesis** â€“ for scalable question generation
- **Expert authoring & validation** â€“ to ensure domain accuracy and risk coverage
- **Interaction & safety task design** â€“ simulating trust boundaries and real agent execution chains
- **Task-aware rubric annotation** â€“ enabling interpretable model evaluation across 3 axes

---

## ğŸ† Leaderboard & Evaluation Platform


CNFinBench is deployed on a **fully automated evaluation platform** built on **OpenCompass**, supporting:

- Unified evaluation of **open-source & closed-source models**
- Task-aware rubrics with LLM-as-Judge protocols
- Real-time leaderboard updates
- Dynamic task and model integration

ğŸ”— **Visit the leaderboard:**  
ğŸ‘‰ https://cnfinbench.opencompass.org.cn/home

<div align="center">   <img src="./assets/image-20260131225448953.png" alt="Platform Overview" width="55%"> </div>

Below is a snapshot of the **current leaderboard** (updated in real time on the platform):

<div align="center">   <img src="./assets/image-20260131230239866.png" alt="Leaderboard Snapshot" width="55%"> </div>

---

## ğŸ“Š Benchmark Scale

- **29 subtasks** across Expertise / Autonomy / Integrity  
- **11,947 single-turn QA instances**  
- **321 multi-turn adversarial dialogues** (4 rounds each)  
- **22 evaluated models** (open-source, closed-source, finance-tuned)

---

## ğŸ’» Code Usage

### Multi-turn Dialogue Evaluation

For **multi-turn adversarial dialogue evaluation**, please refer to the detailed guides in the `multi-turn` directory:

- ğŸ“– **English Guide**: [multi-turn/README.md](multi-turn/README.md)
- ğŸ“– **ä¸­æ–‡æŒ‡å—**: [multi-turn/README_CN.md](multi-turn/README_CN.md)

The multi-turn evaluation pipeline includes:
1. **Generate multi-turn dialogue tests** using scripts in `multi-turn/pred/`
2. **Merge output files** using `multi-turn/pred/merge.py`
3. **Evaluate results** using scripts in `multi-turn/judge/`

For detailed evaluation script documentation:
- ğŸ“– **English**: [multi-turn/judge/README_EN.md](multi-turn/judge/README_EN.md)
- ğŸ“– **ä¸­æ–‡**: [multi-turn/judge/README.md](multi-turn/judge/README.md)

### Quick Start

1. **Install dependencies**:
   ```bash
   cd multi-turn
   pip install -r requirements.txt
   ```

2. **Generate multi-turn dialogues**:
   ```bash
   cd pred
   python main.py --data-dir ../data --output-dir ../output --model-name your_model_name \
       --attack-api-key your_attack_api_key --attack-base-url your_attack_base_url \
       --attack-model-name your_attack_model --defense-api-key your_defense_api_key \
       --defense-base-url your_defense_base_url --defense-model-name your_defense_model
   ```

3. **Merge output files**:
   ```bash
   python merge.py --output-dir ../output
   ```

4. **Evaluate results**:
   ```bash
   cd ..
   python -m judge.evaluate --output-dir ./output \
       --judge-api-key your_judge_api_key --judge-base-url your_judge_base_url \
       --judge-model-name your_judge_model
   ```

For more detailed instructions and examples, please see the [multi-turn README](multi-turn/README.md).

---

## ğŸ“– Citation

If you use CNFinBench, please cite:

```bibtex
@misc{ding2025cnfinbenchbenchmarksafetycompliance,
      title={CNFinBench: A Benchmark for Safety and Compliance of Large Language Models in Finance}, 
      author={Jinru Ding and Chao Ding and Wenrao Pang and Boyi Xiao and Zhiqiang Liu and Pengcheng Chen and Jiayuan Chen and Tiantian Yuan and Junming Guan and Yidong Jiang and Dawei Cheng and Jie Xu},
      year={2025},
      eprint={2512.09506},
      archivePrefix={arXiv},
      primaryClass={cs.CE},
      url={https://arxiv.org/abs/2512.09506}, 
}
